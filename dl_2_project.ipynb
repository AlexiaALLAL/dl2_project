{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7cNVZml79_gK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import urllib.request\n",
        "from keras.datasets import mnist\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose light or full version to run the notebook:\n",
        "- Light will use small values of parameters (number of neurons, number of iterations, ...)\n",
        "- Full will use the values used in the report of this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "notebook_version='full' # 'light' or 'full'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdQlVvIe9_gk"
      },
      "source": [
        "# Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "pC_M3JnY9_gp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def telecharger_alpha_digit(directory='data/'):\n",
        "    \"\"\" Téléchargement des données de l'archive Binary Alpha Digits\n",
        "    à partir de https://cs.nyu.edu/~roweis/data/binaryalphadigs.mat\n",
        "    \"\"\"\n",
        "    \n",
        "    # Création du répertoire de stockage de dataset\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "    # Téléchargement du fichier dans le répertoire choisi\n",
        "    url = \"https://cs.nyu.edu/~roweis/data/binaryalphadigs.mat\"\n",
        "    urllib.request.urlretrieve(url, directory + \"binaryalphadigs.mat\")\n",
        "\n",
        "def lire_alpha_digit(chars):\n",
        "    telecharger_alpha_digit()\n",
        "    alpha_digits = sio.loadmat(\"data/binaryalphadigs.mat\", squeeze_me=True)[\"dat\"]\n",
        "\n",
        "    arr = []\n",
        "    y = []\n",
        "    for i, char in enumerate(chars):\n",
        "        if type(char) == int:\n",
        "            char = str(char)\n",
        "\n",
        "        asc_char = ord(char)\n",
        "        if asc_char >= 65 and asc_char <= 90:\n",
        "            for digit in alpha_digits[asc_char - 55]:\n",
        "                arr.append(digit.flatten())\n",
        "                y.append(asc_char - 55)\n",
        "        elif asc_char >= 48 and asc_char <= 57:\n",
        "            for digit in alpha_digits[asc_char - 48]:\n",
        "                arr.append(digit.flatten())\n",
        "                y.append(asc_char - 48)\n",
        "        else:\n",
        "            raise Exception(\"Char not valid\")\n",
        "    return np.array(arr), np.array(y)\n",
        "\n",
        "def one_hot(y):\n",
        "    unique_labels = np.unique(y)\n",
        "    y_ohe = np.zeros((y.shape[0], len(unique_labels)))\n",
        "    for i in range(y.shape[0]):\n",
        "        y_ohe[i, np.where(unique_labels == y[i])] = 1\n",
        "    return y_ohe\n",
        "\n",
        "def lire_mnist(subsample_size=-1):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # subsample\n",
        "    if subsample_size != -1:\n",
        "        X_train = X_train[:subsample_size]\n",
        "        y_train = y_train[:subsample_size]\n",
        "\n",
        "    # to binary and flatten\n",
        "    X_train_bin = (X_train >= 128).astype(int).reshape(X_train.shape[0], -1)\n",
        "    X_test_bin = (X_test >= 128).astype(int).reshape(X_test.shape[0], -1)\n",
        "\n",
        "    # enc = OneHotEncoder(handle_unknown='ignore')\n",
        "    # # enc.fit(y_train.reshape(-1, 1))\n",
        "    # y_train = enc.fit_transform(y_train.reshape(-1, 1)).todense()\n",
        "    # y_test = enc.fit_transform(y_test.reshape(-1, 1)).todense()\n",
        "    y_train = one_hot(y_train)\n",
        "    y_test = one_hot(y_test)\n",
        "\n",
        "    return X_train_bin, X_test_bin, y_train, y_test\n",
        "\n",
        "def char(idx):\n",
        "    if 0 <= idx <= 9:\n",
        "        return chr(idx + 48)\n",
        "    else:\n",
        "        return chr(idx + 55)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"\n",
        "    Calcule la fonction softmax.\n",
        "    \"\"\"\n",
        "    return (np.exp(x).T / np.sum(np.exp(x), axis=1)).T\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Calcule la fonction sigmoïde.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def calcul_softmax(layer, X):\n",
        "    \"\"\"\n",
        "    Calcule les probabilités sur les unités de sortie à partir de la fonction softmax. Cette fonction retournera les probabilités sur les unités de sortie et prendra en argument un RBM et des données d'entrée.\n",
        "    \"\"\"\n",
        "    return softmax(np.dot(X, layer.W) + layer.b)\n",
        "\n",
        "def err_rate(acc_list):\n",
        "    \"\"\" Calcule une liste de taux d'erreur avec la liste d'accuracy score en entrée \"\"\"\n",
        "    return [1-acc for acc in acc_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-L9JDZ7V9_gv"
      },
      "source": [
        "# Plot functions\n",
        "and save png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory to save plots\n",
        "savepath = 'save/'\n",
        "if not os.path.exists(savepath):\n",
        "    os.makedirs(savepath)\n",
        "    print('Directory \"' + str(savepath) + '\" created')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "357BRstK9_gv"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_data(X, y=None, save=False):\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    plt.figure()\n",
        "    for i in range(9):\n",
        "        plt.subplot(3, 3, i + 1)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.grid(False)\n",
        "        plt.imshow(1-X[perm[i]], cmap=\"gray\")\n",
        "        if y is not None:\n",
        "            plt.xlabel(char(y[perm[i]]))\n",
        "    plt.tight_layout()\n",
        "    if save:\n",
        "        plt.savefig(f\"{savepath + save}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_batched_images(batched_images, variables, save=False):\n",
        "    plt.figure(figsize=(4 * 1.5, len(variables) * 1.5))\n",
        "    for i, images in enumerate(batched_images):\n",
        "        for j, img in enumerate(images):\n",
        "            plt.subplot(len(images), len(batched_images), i + 1 + j * len(variables))\n",
        "            plt.imshow(1-img, cmap=\"gray\")\n",
        "            if j == 0:\n",
        "                plt.title(f\"{variables[i]}\")\n",
        "            plt.axis(\"off\")\n",
        "    plt.tight_layout()\n",
        "    if save:\n",
        "        plt.savefig(f\"{savepath + save}.png\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_preds(X, y_true, y_pred, save=False):\n",
        "    num_cols = 3\n",
        "    num_rows = 3\n",
        "\n",
        "    # random permutation on test set\n",
        "    perm = np.random.permutation(len(y_true))\n",
        "\n",
        "    plt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n",
        "    for i in range(num_rows * num_cols):\n",
        "        idx = perm[i]\n",
        "        true_label = np.argmax(y_true[idx])\n",
        "        predictions_array = y_pred[idx]\n",
        "        predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "        if true_label == predicted_label:\n",
        "            color = \"blue\"\n",
        "        else:\n",
        "            color = \"red\"\n",
        "\n",
        "        # image\n",
        "        plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n",
        "        plt.grid(False)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        plt.imshow(1-X[idx].reshape(28, 28), cmap=plt.cm.gray)\n",
        "        plt.xlabel(\n",
        "            \"truth={}, pred={}, score={:2.0f}\".format(\n",
        "                true_label,\n",
        "                predicted_label,\n",
        "                100 * np.max(predictions_array),\n",
        "                color=color,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # probs\n",
        "        plt.subplot(num_rows, 2 * num_cols, 2 * i + 2)\n",
        "        plt.grid(False)\n",
        "        plt.xticks([])\n",
        "        plt.yticks([])\n",
        "        bp = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "        bp[predicted_label].set_color(\"red\")\n",
        "        bp[true_label].set_color(\"blue\")\n",
        "        plt.ylim([0, 1])\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(f\"{savepath + save}.png\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "def plot_DNN(x_axis, acc_1, acc_2=None, acc_1_train=None, acc_2_train=None, x_label='', top=None, save=False):\n",
        "    \"\"\"\n",
        "    Plot DNN results (for a set of values for a given parameter to be tested)\n",
        "    \n",
        "    Params:\n",
        "    -------\n",
        "    x_axis: list of values that were tested\n",
        "    acc_1: list of accuracy scores on the test set (pretrain) \n",
        "    acc_2: list of accuracy scores on the test set (train only) \n",
        "    acc_1_train: list of accuracy scores on the train set (pretrain) \n",
        "    acc_2_train: list of accuracy scores on the train set (train only) \n",
        "    x_label: name of the variable that was tested\n",
        "    top: top limit of y axis\n",
        "    save: name of the file to save the plot (without extension), None if no \n",
        "    Returns:\n",
        "    --------\n",
        "    Nothing but plot the graph and if requested save it in a .png file\n",
        "    \"\"\"\n",
        "    plt.plot(\n",
        "        x_axis,\n",
        "        err_rate(acc_1),\n",
        "        label=\"Pretrain + train (test set)\",\n",
        "        color='blue',\n",
        "        marker=\"o\",\n",
        "        markersize=8,\n",
        "    )\n",
        "    if acc_1_train != None:\n",
        "        plt.plot(\n",
        "            x_axis,\n",
        "            err_rate(acc_1_train),\n",
        "            label=\"Pretrain + train (train set)\",\n",
        "            color='blue',\n",
        "            linestyle='dashed', \n",
        "            linewidth=.5\n",
        "        )\n",
        "    if acc_2 != None:\n",
        "        plt.plot(\n",
        "            x_axis,\n",
        "            err_rate(acc_2),\n",
        "            label=\"Train only (test set)\",\n",
        "            color='orange',\n",
        "            marker=\"o\",\n",
        "            markersize=8\n",
        "        )\n",
        "    if acc_2_train != None:\n",
        "        plt.plot(\n",
        "            x_axis,\n",
        "            err_rate(acc_2_train),\n",
        "            label=\"Train only (train set)\",\n",
        "            color='orange',\n",
        "            linestyle='dashed',\n",
        "            linewidth=.5    \n",
        "        )\n",
        "    plt.xticks(x_axis)\n",
        "    plt.gca().set_ylim(bottom=0)\n",
        "    if top!=None:\n",
        "        plt.gca().set_ylim(top=top)\n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(\"Error rate\")\n",
        "    plt.legend()\n",
        "    if save:\n",
        "        plt.savefig(savepath + save + \".png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAnlioCM9_g0"
      },
      "source": [
        "# RBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "leCPIk8v9_g1"
      },
      "outputs": [],
      "source": [
        "class RBM:\n",
        "    def __init__(self, p, q):\n",
        "        self.W = np.random.normal(0, 0.01, (p, q))\n",
        "        self.a = np.zeros(p)\n",
        "        self.b = np.zeros(q)\n",
        "\n",
        "\n",
        "def init_RBM(p, q):\n",
        "    \"\"\"\n",
        "    Construit et initialise les poids et les biais d'un RBM. Cette fonction retournera une structure RBM avec des poids et biais initialisés.\n",
        "    On initialisera les biais à 0 tandis que les poids seront initialisés aléatoirement suivant une loi normale centrée, de variance égale à 0.01.\n",
        "    \"\"\"\n",
        "    rbm = RBM(p, q)\n",
        "    return rbm\n",
        "\n",
        "\n",
        "def entree_sortie_RBM(rbm, X):\n",
        "    \"\"\"\n",
        "    Calcule la valeur des unités de sortie d'un RBM à partir des données d'entrée et de la fonction sigmoïde.\n",
        "    \"\"\"\n",
        "    return sigmoid(np.dot(X, rbm.W) + rbm.b)\n",
        "\n",
        "\n",
        "def sortie_entree_RBM(rbm, H):\n",
        "    \"\"\"\n",
        "    Calcule la valeur des unités d'entrée d'un RBM à partir des données de sortie et de la fonction sigmoïde.\n",
        "    \"\"\"\n",
        "    return sigmoid(np.dot(H, rbm.W.T) + rbm.a)\n",
        "\n",
        "\n",
        "def train_RBM(rbm, X, epochs, lr, batch_size):\n",
        "    \"\"\"\n",
        "    Apprend un RBM par l'algorithme Contrastive-Divergence-1. Cette fonction retournera un structure\n",
        "    RBM et prendra en argument une structure RBM, le nombre d'itérations de la descente de gradient (epochs),\n",
        "    le learning rate, la taille du mini-batch, des données d'entrées...\n",
        "    \"\"\"\n",
        "    print(\"Training RBM...\")\n",
        "    n = X.shape[0]\n",
        "    p, q = rbm.W.shape\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        X_copy = np.copy(X)\n",
        "        np.random.shuffle(X_copy)\n",
        "        for i in range(0, n, batch_size):\n",
        "            v_0 = X[i : i + batch_size]\n",
        "            tb = len(v_0)\n",
        "            es_v_0 = entree_sortie_RBM(rbm, v_0)\n",
        "            h_0 = (np.random.uniform(size=(tb, q)) < es_v_0).astype(\"float\")\n",
        "            v_1 = (\n",
        "                np.random.uniform(size=(tb, p)) < sortie_entree_RBM(rbm, h_0)\n",
        "            ).astype(\"float\")\n",
        "            h_1 = entree_sortie_RBM(rbm, v_1)\n",
        "            rbm.W += lr / tb * (np.dot(v_0.T, es_v_0) - np.dot(v_1.T, h_1))\n",
        "            rbm.a += lr / tb * np.sum(v_0 - v_1, axis=0)\n",
        "            rbm.b += lr / tb * np.sum(es_v_0 - h_1, axis=0)\n",
        "    return rbm\n",
        "\n",
        "\n",
        "def generer_image_RBM(rbm, n_iter, n_image):\n",
        "    \"\"\"\n",
        "    Génère des échantillons suivant un RBM. Cette fonction retournera et affichera les images générées et prendra en argument une structure de type RBM, le nombre d'itérations à utiliser dans l'échantillonneur de Gibbs et le nombre d'images à générer.\n",
        "    \"\"\"\n",
        "    p, q = rbm.W.shape\n",
        "    proba = np.random.random()\n",
        "    X = (np.random.uniform(size=(n_image, p)) < proba).astype(\"float\")\n",
        "    for i in range(n_iter):\n",
        "        H = (np.random.uniform(size=(n_image, q)) < entree_sortie_RBM(rbm, X)).astype(\n",
        "            \"float\"\n",
        "        )\n",
        "        X = (np.random.uniform(size=(n_image, p)) < sortie_entree_RBM(rbm, H)).astype(\n",
        "            \"float\"\n",
        "        )\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-cxh8ot9_g2"
      },
      "source": [
        "# DBN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Kb6o0zF79_g6"
      },
      "outputs": [],
      "source": [
        "class DBN:\n",
        "    def __init__(self, n_layers):\n",
        "        self.layers = []\n",
        "        for i in range(len(n_layers) - 1):\n",
        "            self.layers.append(RBM(n_layers[i], n_layers[i + 1]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.layers)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.layers[i]\n",
        "\n",
        "    def __setitem__(self, i, layer):\n",
        "        self.layers[i] = layer\n",
        "\n",
        "\n",
        "def init_DBN(n_layers):\n",
        "    \"\"\"\n",
        "    Construit et initialise les poids et les biais d'un DBN. Cette fonction retournera un DBN avec des poids et biais initialisés.\n",
        "    On initialisera les biais à 0 tandis que les poids seront initialisés aléatoirement suivant une loi normale centrée, de variance égale à 0.01.\n",
        "    \"\"\"\n",
        "    dbn = DBN(n_layers)\n",
        "    return dbn\n",
        "\n",
        "\n",
        "def train_DBN(dbn, X_init, epochs, lr, batch_size):\n",
        "    \"\"\"\n",
        "    Apprend un DBN par la méthode Greedy Layer-Wise. Cette fonction retournera un DBN pré-entrainé et prendra en argument un DBN, le nombre d'itérations de la descente de gradient, le learning rate, la taille du mini-batch, des données d'entrées.\n",
        "    \"\"\"\n",
        "    print(\"Training DBN...\")\n",
        "    X = X_init\n",
        "    for rbm in dbn:\n",
        "        rbm = train_RBM(rbm, X, epochs, lr, batch_size)\n",
        "        X = entree_sortie_RBM(rbm, X)\n",
        "    return dbn\n",
        "\n",
        "\n",
        "def generer_image_DBN(dbn, n_iter, n_image):\n",
        "    \"\"\"\n",
        "    Génère des échantillons suivant un DBN. Cette fonction retournera et affichera les images générées et prendra en argument un DBN pré-entrainé, le nombre d'itérations à utiliser dans l'échantillonneur de Gibbs et le nombre d'images à générer.\n",
        "    \"\"\"\n",
        "    X = generer_image_RBM(dbn[-1], n_iter, n_image)\n",
        "    for i in range(len(dbn) - 2, -1, -1):\n",
        "        p, q = dbn[i].W.shape\n",
        "        X = (\n",
        "            np.random.uniform(size=(n_image, p)) < sortie_entree_RBM(dbn[i], X)\n",
        "        ).astype(\"float\")\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU0xPna99_g6"
      },
      "source": [
        "# DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yCcY-T3v9_g7"
      },
      "outputs": [],
      "source": [
        "class FC:\n",
        "    def __init__(self, p, q):\n",
        "        self.W = np.random.normal(0, 0.1, (p, q))\n",
        "        self.b = np.zeros(q)\n",
        "\n",
        "\n",
        "class DNN(DBN):\n",
        "    def __init__(self, n_layers):\n",
        "        self.layers = []\n",
        "        for i in range(len(n_layers) - 2):\n",
        "            self.layers.append(RBM(n_layers[i], n_layers[i + 1]))\n",
        "        self.layers.append(FC(n_layers[-2], n_layers[-1]))\n",
        "\n",
        "\n",
        "def init_DNN(n_layers):\n",
        "    \"\"\"\n",
        "    Construit et initialise les poids et les biais d'un DNN. Cette fonction retournera un DNN avec des poids et biais initialisés.\n",
        "    On initialisera les biais à 0 tandis que les poids seront initialisés aléatoirement suivant une loi normale centrée, de variance égale à 0.01.\n",
        "    \"\"\"\n",
        "    dnn = DNN(n_layers)\n",
        "    return dnn\n",
        "\n",
        "\n",
        "def pretrain_DNN(dnn, X_init, epochs, lr, batch_size):\n",
        "    \"\"\"\n",
        "    Pré-entraine un DNN. Cette fonction retournera un DNN pré-entrainé et prendra en argument un DNN, le nombre d'itérations de la descente de gradient, le learning rate, la taille du mini-batch, des données d'entrées.\n",
        "    \"\"\"\n",
        "    print(\"Pretraining DNN...\")\n",
        "    X = X_init\n",
        "    for rbm in dnn[:-1]:\n",
        "        rbm = train_RBM(rbm, X, epochs, lr, batch_size)\n",
        "        X = entree_sortie_RBM(rbm, X)\n",
        "    return dnn\n",
        "\n",
        "\n",
        "def entree_sortie_reseau(dnn, X):\n",
        "    \"\"\"\n",
        "    Calcule les sorties sur chaque couche du réseau (couche d'entrée inclue) ainsi que les probabilités sur les unités de sortie. Cette fonction retournera une liste contenant les sorties sur chaque couche du réseau ainsi que les probabilités sur les unités de sortie et prendra en argument un DNN et des données d'entrée.\n",
        "    \"\"\"\n",
        "    sorties = [X]\n",
        "    for rbm in dnn[:-1]:\n",
        "        sorties.append(entree_sortie_RBM(rbm, sorties[-1]))\n",
        "    sorties.append(calcul_softmax(dnn[-1], sorties[-1]))\n",
        "    return sorties\n",
        "\n",
        "\n",
        "def retropropagation(dnn, X, y, epochs, lr, batch_size, verbose):\n",
        "    \"\"\"\n",
        "    Estime les poids/biais du réseau à partir de données labellisées. Cette fonction retournera un DNN et prendra en argument un DNN, le nombre d'itérations de la descente de gradient, le learning rate, la taille du mini-batch, des données d'entrée, leur label,...\n",
        "    \"\"\"\n",
        "    print(\"Training DNN...\")\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        for i in range(0, X.shape[0], batch_size):\n",
        "            X_batch = X[i : i + batch_size]\n",
        "            y_batch = y[i : i + batch_size]\n",
        "            tb = len(X_batch)\n",
        "            sortie = entree_sortie_reseau(\n",
        "                dnn,\n",
        "                X_batch,\n",
        "            )\n",
        "            # compute dL/dZ\n",
        "            d_Z = sortie[-1] - y_batch\n",
        "\n",
        "            for j in range(len(dnn) - 1, -1, -1):\n",
        "                grad_W = 1 / tb * np.dot(sortie[j].T, d_Z)\n",
        "                grad_b = 1 / tb * np.sum(d_Z, axis=0)\n",
        "\n",
        "                # update W and b\n",
        "                dnn.layers[j].W -= lr * grad_W\n",
        "                dnn.layers[j].b -= lr * grad_b\n",
        "\n",
        "                if j == 0:  # no need to compute at last iteration\n",
        "                    break\n",
        "\n",
        "                # compute dL/dA and dL/dZ\n",
        "                d_A = np.dot(d_Z, dnn[j].W.T)\n",
        "                d_Z = d_A * sortie[j] * (1 - sortie[j])\n",
        "            cross_entropy = -np.mean(np.sum(y_batch * np.log(sortie[-1]), axis=1))\n",
        "        if verbose == 2:\n",
        "            print(f\"Epoch {epoch} - CE:\", cross_entropy)\n",
        "    return dnn\n",
        "\n",
        "\n",
        "def test_DNN(dnn, X, y):\n",
        "    \"\"\"\n",
        "    Teste les performances du réseau appris. Cette fonction retournera le taux d'erreur et prendra en argument un DNN appris, un jeu de données test, et les vrais labels associés.\n",
        "    \"\"\"\n",
        "    sortie = entree_sortie_reseau(dnn, X)\n",
        "    # Compute cross entropy\n",
        "    cross_entropy = -np.mean(np.sum(y * np.log(sortie[-1]), axis=1))\n",
        "    return sortie[-1], cross_entropy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zN5tOYnZ9_hD"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vOx6Q5Zq9_hD",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "def RBM_main(\n",
        "    X,\n",
        "    width: int,\n",
        "    height: int,\n",
        "    q: int,\n",
        "    epochs: int,\n",
        "    lr,\n",
        "    batch_size: int,\n",
        "    n_images: int = 9,\n",
        "):\n",
        "\n",
        "    rbm = init_RBM(p=width * height, q=q)\n",
        "    \n",
        "    train_RBM(rbm, X, epochs, lr, batch_size)\n",
        "\n",
        "    X_gen = generer_image_RBM(rbm, 100, n_images).reshape(-1, width, height)\n",
        "    return X_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "KTnblAoD9_hD"
      },
      "outputs": [],
      "source": [
        "def DBN_main(\n",
        "    X,\n",
        "    n_layers,\n",
        "    width: int,\n",
        "    height: int,\n",
        "    epochs: int,\n",
        "    lr,\n",
        "    batch_size: int,\n",
        "    n_images: int = 9,\n",
        "):\n",
        "    dbn = init_DBN(n_layers)\n",
        "\n",
        "    train_DBN(dbn, X, epochs, lr, batch_size)\n",
        "\n",
        "    X_gen = generer_image_DBN(dbn, 100, n_images).reshape(-1, width, height)\n",
        "    return X_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qMVfEf0_9_hE"
      },
      "outputs": [],
      "source": [
        "def DNN_main(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    n_layers,\n",
        "    lr,\n",
        "    batch_size,\n",
        "    pretrain_iter,\n",
        "    train_iter,\n",
        "    show_preds=False,\n",
        "    verbose=1,\n",
        "):\n",
        "    # Initialisation du DNN\n",
        "    dnn = init_DNN(n_layers)\n",
        "\n",
        "    # Pré-entrainement du DNN\n",
        "    \n",
        "    if pretrain_iter:\n",
        "        print('pré-entrainement')\n",
        "        dnn = pretrain_DNN(dnn, X_train, pretrain_iter, lr, batch_size)\n",
        "\n",
        "    # Entrainement du DNN\n",
        "    print('entrainement')\n",
        "    dnn = retropropagation(\n",
        "        dnn, X_train, y_train, train_iter, lr, batch_size, verbose\n",
        "    )\n",
        "\n",
        "    # Test du DNN\n",
        "    print('train')\n",
        "    y_train_pred, cross_entropy_train = test_DNN(dnn, X_train, y_train)\n",
        "    train_score = accuracy_score(\n",
        "        np.argmax(y_train, axis=1), np.argmax(y_train_pred, axis=1)\n",
        "    )\n",
        "    # Test du DNN\n",
        "    print('test')\n",
        "    y_pred, cross_entropy = test_DNN(dnn, X_test, y_test)\n",
        "    test_score = accuracy_score(\n",
        "        np.argmax(y_test, axis=1), np.argmax(y_pred, axis=1)\n",
        "    )\n",
        "\n",
        "    if verbose >= 1:\n",
        "        print(\"accuracy_score (train)\", train_score)\n",
        "        print(\"accuracy_score (test)\", test_score)\n",
        "\n",
        "    if show_preds:\n",
        "        plot_preds(X_test, y_test, y_pred, save=\"mnist_preds_with_prob\")\n",
        "\n",
        "    return cross_entropy, test_score, train_score, dnn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVwGq4KJ9_hF"
      },
      "source": [
        "# Etude sur Binary AlphaDigit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VUTGSxYx9_hF"
      },
      "outputs": [],
      "source": [
        "alphadigit = [str(i) for i in range(10)] + [chr(i) for i in range(65, 91)]\n",
        "\n",
        "X, y = lire_alpha_digit(chars=alphadigit)\n",
        "height = 20\n",
        "width = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "EFRCXXUl9_hG",
        "outputId": "e4000432-8c4e-4835-a313-569b475c508b"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 9 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plot_data(X.reshape(-1, height, width), y, save=\"binary_alphadigit\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "NYbyM7x_9_hH"
      },
      "source": [
        "# Nombre de neurones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRrLOz5V9_hI",
        "outputId": "08a17c5f-5a3d-463b-8eec-c297a49396eb",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 100 neurons by layer ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 53.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:05<00:00, 19.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 550.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 300 neurons by layer ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:03<00:00, 27.94it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 72.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 114.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 500 neurons by layer ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 55.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 55.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 40.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 700 neurons by layer ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 42.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 44.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:04<00:00, 22.45it/s]\n"
          ]
        }
      ],
      "source": [
        "digit = ['A', 'B']\n",
        "# digit = [str(i) for i in range(2, 10)]\n",
        "X, y = lire_alpha_digit(chars=digit)\n",
        "\n",
        "n_neurons = range(100, 701, 200)\n",
        "\n",
        "epochs = 100\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "\n",
        "rbm_images = []\n",
        "dbn_images = []\n",
        "\n",
        "for n in n_neurons:\n",
        "    print(f\"--- {n} neurons by layer ---\")\n",
        "\n",
        "    X_gen_rbm = RBM_main(\n",
        "        X,\n",
        "        height,\n",
        "        width,\n",
        "        n,\n",
        "        epochs,\n",
        "        lr,\n",
        "        batch_size,\n",
        "    )\n",
        "\n",
        "    idxs = np.random.choice(X_gen_rbm.shape[0], 4, replace=False)\n",
        "    rbm_images.append(X_gen_rbm[idxs])\n",
        "\n",
        "    X_gen_dbn = DBN_main(\n",
        "        X,\n",
        "        [320, n, n],\n",
        "        height,\n",
        "        width,\n",
        "        epochs,\n",
        "        lr,\n",
        "        batch_size,\n",
        "    )\n",
        "\n",
        "    idxs = np.random.choice(X_gen_dbn.shape[0], 4, replace=False)\n",
        "    dbn_images.append(X_gen_dbn[idxs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xMyERO4U9_hI",
        "outputId": "e53fec5b-dee8-40d8-8a2d-3176e10df7d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RBM\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBN\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"RBM\")\n",
        "plot_batched_images(rbm_images, n_neurons, save=\"rbm_n_neurons\")\n",
        "\n",
        "print(\"DBN\")\n",
        "plot_batched_images(dbn_images, n_neurons, save=\"dbn_n_neurons\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "fONZAFUC9_hJ"
      },
      "source": [
        "# Nombre de couches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O8qRRJT9_hJ",
        "outputId": "fd34f5f3-d06e-4dba-c2bd-8ed1798eb805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1 layers ---\n",
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 115.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 2 layers ---\n",
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 58.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 118.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 3 layers ---\n",
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:02<00:00, 46.27it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 166.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 149.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 4 layers ---\n",
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 103.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:01<00:00, 71.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 126.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 144.30it/s]\n"
          ]
        }
      ],
      "source": [
        "digit = ['A', 'B']\n",
        "X, y = lire_alpha_digit(chars=digit)\n",
        "\n",
        "n_layers = range(1, 5)\n",
        "\n",
        "epochs = 100\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "q = 200\n",
        "\n",
        "dbn_images = []\n",
        "\n",
        "for n in n_layers:\n",
        "    print(f\"--- {n} layers ---\")\n",
        "    layers = [200] * n\n",
        "\n",
        "    X_gen_dbn = DBN_main(\n",
        "        X,\n",
        "        [320] + layers,\n",
        "        height,\n",
        "        width,\n",
        "        epochs,\n",
        "        lr,\n",
        "        batch_size,\n",
        "    )\n",
        "\n",
        "    idxs = np.random.choice(X_gen_dbn.shape[0], 4, replace=False)\n",
        "    dbn_images.append(X_gen_dbn[idxs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "zXmCclYD9_hJ",
        "outputId": "64649bbb-45ff-42c6-fd74-ac8a988d5b43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBN\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAJOCAYAAACOd7w2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZGUlEQVR4nO3dTahlR7kG4NraippGSEirRG0aERQRHAihnQUHIgbRuYoiQTGJ6CAKxp90MEomGfiDoggRiYgkjgIJgiT0zIk/I535i4JNiy3pHiiScwch916Ts3f2Wqf2W1VrPQ84SSdn16r69uqXsr5Tm6Ojo6MCABD0otYDAADWRwABAOIEEAAgTgABAOIEEAAgTgABAOIEEAAgTgABAOIEEAAgbhUB5Kmnniqf/exny7ve9a5y5syZstlsyoULF1oPi0498cQT5aMf/Wh585vfXK677rry2te+trzvfe8rv/jFL1oPjY79+te/Lrfeems5e/ZsefnLX15uuOGG8o53vKM89NBDrYfGIL73ve+VzWZTTp8+3XooEasIIH//+9/Ld7/73fKvf/2rvP/97289HDr37W9/u/zhD38on/rUp8pjjz1Wvva1r5VLly6V8+fPlyeeeKL18OjUlStXyutf//ry1a9+tTz22GPlBz/4QTl37lz50Ic+VO67777Ww6Nzf/nLX8pdd91VbrrpptZDidms4S6YZx9xs9mUy5cvlzNnzpR77rnHLgjHunTpUnnVq171X//s6tWr5Y1vfGN561vfWn72s581GhkjOn/+fPnrX/9a/vSnP7UeCh1773vfWzabTbnhhhvKI488Uq5evdp6SAe3ih2QzWZTNptN62EwiOeGj1JKOX36dHnLW95S/vznPzcYESO78cYby6lTp1oPg4499NBD5eLFi+Vb3/pW66FE+VbAHv75z3+WX/7yl+Wd73xn66HQuaeffro8/fTT5R//+Ed5+OGHy09/+tPyzW9+s/Ww6NSlS5fKpz/96XL//feX173uda2HEyWAwB7uuOOOcu3atfL5z3++9VDo3O23316+853vlFJKeelLX1q+/vWvl49//OONR0Wvbr/99vKmN72pfOITn2g9lDgBBF7AF7/4xfLDH/6wfOMb3yhvf/vbWw+Hzt19993ltttuK5cuXSqPPvpoufPOO8u1a9fKXXfd1XpodOYnP/lJefTRR8uvfvWrVR4TEEBgh3vvvbfcd9995Stf+Uq58847Ww+HAZw9e7acPXu2lFLKe97znlJKKZ/73OfKhz/84XLmzJmWQ6MjV69eLXfccUf55Cc/WW666aZy5cqVUkop//73v0spz3RVveQlLynXXXddw1Ee1ioOocIc9957b7lw4UK5cOFCufvuu1sPh0HdfPPN5T//+U/53e9+13oodOTy5cvlb3/7W3nggQfK9ddf/7//+9GPflSuXbtWrr/++vKBD3yg9TAPyg4IHOPLX/5yuXDhQvnCF75Q7rnnntbDYWBPPvlkedGLXlTe8IY3tB4KHXnNa15Tnnzyyef98/vvv79cvHixPP744+XGG29sMLKc1QSQxx9/vFy7dq089dRTpZRSfvOb35RHHnmklPLMNukrXvGKlsOjIw888ED50pe+VN797neXW2+9tfz85z//rz8/f/58o5HRs4997GPlla98Zbn55pvLq1/96nL58uXy8MMPlx//+MflM5/5jP/7hf/yspe9rNxyyy3P++ff//73y4tf/OJj/2xpVvGLyEop5dy5c+WPf/zjsX/2+9//vpw7dy47ILp1yy23lIsXL27985V8ZZjowQcfLA8++GD57W9/W65cuVJOnz5d3va2t5XbbrutfPCDH2w9PAbxkY98ZDW/iGw1AQQA6IdDqABAnAACAMQJIABAnAACAMQJIABAnAACAMQJIABA3GJ/E2qvNwv6tSvtTa2NXWvWss7UUn211rN1zaiN8eyqi23rOXot2QEBAOIEEAAgTgABAOIEEAAgrqvL6Ho9ONpaR0vUHTVzPDWz3RpqZs76b5sXtZQxYl2etDbsgAAAcQIIABAngAAAcQIIABAngAAAcQIIABDX5C6Y0X9//bNSbVNrao/r8W6VOXc0bKNm5un1noxtarbBzjFiS+daWJv/YwcEAIgTQACAOAEEAIgTQACAOAEEAIg7cRdM6kTvqKf3a1jas49WMzXnf9fPcjp+uznzluiOqfkZrTtnqCtRZ4nPPiQ7IABAnAACAMQJIABAnAACAMQJIABA3N5dMEu5v6WmVEfD0u71qDlvo84B08z5PqU6UQ75c0oZp6NhrWqtzxrfZXZAAIA4AQQAiBNAAIA4AQQAiBNAAIA4AQQAiNu7DbfmRUlrbDdao5aXMcE2o9WSCwzZZvRfj2EHBACIE0AAgDgBBACIE0AAgDgBBACI27sLZpual0SNdjq9Jifdd9NRM82oz6jWn8+ctJd4/7Re5xbvDDsgAECcAAIAxAkgAECcAAIAxAkgAEDcibtgRtP6pPEco3Y01Lw/KPH5cz478Rm1Ppsx6ZBbltZr1tN7ww4IABAngAAAcQIIABAngAAAcQIIABAngAAAcV214e5qT+r1Ap9temp16k1qzVrWRuKz53xf1sQcMNVovwZgjp4ug7UDAgDECSAAQJwAAgDECSAAQJwAAgDEddUFsyRTTzSv6cT+nMu1eu2Cavn5a6qZOeac9u+pQ2AfNb8XvT7j0rR+Z011yG47OyAAQJwAAgDECSAAQJwAAgDECSAAQNzm6ITHWFv+7vyU1qeWe52XQ6jZPVSro6HmKfBULS2tZlL3RI02b2t4/8615vf2KO8ZOyAAQJwAAgDECSAAQJwAAgDECSAAQJwAAgDEddWGu0uPLWI12/zW0DL4LDVzvMSFez3Oyf/XunUyQet2hnmbpsV7xg4IABAngAAAcQIIABAngAAAcQIIABB3qsWHJk77J8w5/TzaM1LXnMvzyEm8m3pd51qXNy6ROTgMOyAAQJwAAgDECSAAQJwAAgDECSAAQJwAAgDEHbQNd2pLm1Yn1mwp7enUpXWbhBbvHzsgAECcAAIAxAkgAECcAAIAxAkgAEDcQbtgpp6e3fXv65BZDqf62WYpnUDeV2Oybll2QACAOAEEAIgTQACAOAEEAIgTQACAuIN2wfB8SznlfxJrelaWQXdEv7xPxmUHBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDhtuGFaxthGbWSM1lKbqovR5uUQts31kuamp/eMHRAAIE4AAQDiBBAAIE4AAQDiBBAAIG6YLpjRTif3dNK4lV7nYGot7XqOqf9Nr/W6JnPWs6VdY5r6HZvzs3qck560nLfW79iTPqMdEAAgTgABAOIEEAAgTgABAOIEEAAgbpgumG1qngKe0wVx6M8e2ZxnqjnXrU+IH9oSayZhzfM2WgfQPmq+Z2p2wtUa05LZAQEA4gQQACBOAAEA4gQQACBOAAEA4gQQACBu+DbcmtbYBtVCr/M8Z1xT/5uaF9st0ZqedV9z5kQtZb7Pc/+blnq6QNMOCAAQJ4AAAHECCAAQJ4AAAHECCAAQtzka9QYiAGBYdkAAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgLhTrQfw/202m61/dnR0VO1nTTX1s9mt5trUZJ3ZJlGz6o9aRvn7zw4IABAngAAAcQIIABAngAAAcV0dQt2l14OLLMcoB7dYl5qH8+lXr3/HHbL+7IAAAHECCAAQJ4AAAHECCAAQJ4AAAHECCAAQtzk6YR/NnBadXtuNptICt1uvba3bxpWoVzXTtx7fTWpmWXqssbm04QIAwxFAAIA4AQQAiBNAAIA4AQQAiDtoF8yaObm+3WiXa6VqvMdnH9ka3k1qpl9T6y/R6VebLhgAYDgCCAAQJ4AAAHECCAAQJ4AAAHGn9v0XE6dqd52onXp/x9SfU1ut8S7RaHMwpy7JGe1uHzWzLEtfz0O+r+2AAABxAggAECeAAABxAggAECeAAABxAggAELd3G26ipW3O59f8OUtvp2I3rdNjav1umqrmeNXsOiTWuUXN2AEBAOIEEAAgTgABAOIEEAAgTgABAOI2Rwc8+tr6ArtalvIcS7RrbXqsDes8z2jddrt4n/Sr5dq07trSBQMArIIAAgDECSAAQJwAAgDECSAAQNzeXTBrOLnd8hRy62dPSnWutD5Vfpw1rfMcc+68mPrftO6cmqpmHff4fCPo8V1SyvjraQcEAIgTQACAOAEEAIgTQACAOAEEAIgTQACAuK7acHfpsd1oSRdkJc2Ztzlz0Lpmj7O0tezZGtpXXYZY11Iuo6v5vjxkzdgBAQDiBBAAIE4AAQDiBBAAIE4AAQDiTrUewAhad1O0OJ3cm6XMQaoDaE2W3glS8/2j/nJazlvrTsN9f5YdEAAgTgABAOIEEAAgTgABAOIEEAAg7sRdMLtOuya6R6aewt01ppqnlhO/75/dEqfQe6zxkSXuw0i9A+hXj+/h1vddtZgTOyAAQJwAAgDECSAAQJwAAgDECSAAQJwAAgDEbY727P3psW2plHW3uy6xZXApF4u1rr9e5+WFtJ63babO55xW36U8ey9az+do85ZogX8uOyAAQJwAAgDECSAAQJwAAgDECSAAQNzel9HVvNhp9At04IX0erHUaCfzD6HmBZpz1sw7a1l8p+azAwIAxAkgAECcAAIAxAkgAECcAAIAxAkgAEDc3m24rdvNprb01mwBbt1SyXZz2sB7NdolZUvUsma8Z2ipxfvHDggAECeAAABxAggAECeAAABxAggAELd3F0xNNU+a9/qzyFjDmumOmWe0jpaan5Pq9luLNc9Nzcsbn8sOCAAQJ4AAAHECCAAQJ4AAAHECCAAQ16QLpqURT6evydR5W/Np/0OeTmc/S6mlkSXuHFszd8EAAIsigAAAcQIIABAngAAAcQIIABAngAAAcU3acLe19WiBYpuabc3qbx3WvM5zvi9rnq9nrWEOemrRtwMCAMQJIABAnAACAMQJIABAnAACAMSt7jK61lxS169eT8CvqTZ8P+pY03ypmedLPftJ3412QACAOAEEAIgTQACAOAEEAIgTQACAuL27YJw0hpPzfclp3b3Ukvf1PC1rpte1OeSc2AEBAOIEEAAgTgABAOIEEAAgTgABAOIEEAAgbu823F5bhEZjHuva1SJWa65HXLNRW1BHnOsemcd574Y1z1uLd4YdEAAgTgABAOIEEAAgTgABAOIEEAAgbnM06nF5AGBYdkAAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIO9V6ACe12Wyq/ayjo6NqP4t1m1OX6m8dar6zptpVY9vGpS7rarn+u7RYZzsgAECcAAIAxAkgAECcAAIAxDU5hNrrIRwHWjlOr/XKPLvWc+r3drTaGG28vVjDvLU4OG8HBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgLjh74LplXsVxrOGVjt2UwPrtoZfxVDzGU/695wdEAAgTgABAOIEEAAgTgABAOIEEAAg7sRdME6N0yudSOsw9R00Z/1HqyXv5bp6Xec5tj1Li5qxAwIAxAkgAECcAAIAxAkgAECcAAIAxHV1F8yuk8ZOda9b4v6CbWp2TdDenLVZUhcE261hnXu678YOCAAQJ4AAAHECCAAQJ4AAAHECCAAQJ4AAAHF7t+G2bius1R6Veo41tHNBiu/t8Vq/l+lTT622u9gBAQDiBBAAIE4AAQDiBBAAIE4AAQDiTnwZXa8XyG377NR453w+bWum17VRS3W/t2uaN5Yh8V5s8b2wAwIAxAkgAECcAAIAxAkgAECcAAIAxJ24C2bX6dxtp2oT3Sa1/n2yEjWTOO1d8zl0beQkOo56fQeps3ladqiM/j6xAwIAxAkgAECcAAIAxAkgAECcAAIAxAkgAEDc5mjPnpxEG2TNlqLWbZsuyJpnzfPm0rntUq2rifdJwpznUGfjrXPCIevCDggAECeAAABxAggAECeAAABxAggAENekC6ampZxa32Vpp9Nbr83S5nMNWtfMUqj9nJpdbS3rf9d4T/qMdkAAgDgBBACIE0AAgDgBBACIE0AAgLjhu2DWbNQT7WqpjlHXf47WNaPbbt1q3lM2mkPeH2QHBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgLhTrQcAhzK1RWzOxUot2zDX3Bq4j8SzJj6j5jovqW04qdd3wDZzWmfnPIfL6ACA4QggAECcAAIAxAkgAECcAAIAxA1zGV2Pp/drzsmcU9Y9zsk+1lBLiWdUM/OMOgfHMS8Za57nQ3bb2QEBAOIEEAAgTgABAOIEEAAgTgABAOIEEAAgrqvL6EZrT6p5QVHr1tRRtW6p7bVmex0Xdbl0jpHZAQEA4gQQACBOAAEA4gQQACBOAAEA4vbugkmcth6x22Abp9OXc4HTnM+e899MvUBuSd8X5pn6HVMX9MQOCAAQJ4AAAHECCAAQJ4AAAHECCAAQ19VdMGugO2bd5qzz1O4YnjFa95B1pqUWfwfZAQEA4gQQACBOAAEA4gQQACBOAAEA4gQQACBu7zZcbaLAoa3hPVOz3VZbf11rmLeentEOCAAQJ4AAAHECCAAQJ4AAAHECCAAQN8xldFNP7iYuoprzs3o6gXxo5oBadn03a70b5vysOZ8x9bPnjNcFduswp157ei/bAQEA4gQQACBOAAEA4gQQACBOAAEA4jZHex6X1rnQjhPty1Hze7TEuhj9VH/vllgz26Q6Hmt9Ru3Pr+WQNWMHBACIE0AAgDgBBACIE0AAgDgBBACIE0AAgLi923C3/oBQW6GWuudbU0vdUtS8wHBNl5St+fs/6pqlrLk25uipnuyAAABxAggAECeAAABxAggAECeAAABxJ+6CAQCYyg4IABAngAAAcQIIABAngAAAcQIIABAngAAAcQIIABAngAAAcQIIABAngAAAcQIIABAngAAAcQIIABAngAAAcadaD2Bfm82mys85Ojo6+GfM/Xz6lKiLOdRSv2rWjHVmm9H/zrIDAgDECSAAQJwAAgDECSAAQNzm6IAnTHo9vDcah9C2U2PHUzN19VpnNdd52zOqpbp6raU5TlobdkAAgDgBBACIE0AAgDgBBACIE0AAgDgBBACIO3Eb7pJaikYzantc65oZdd6ea848jvrsu5516jO1rr+EUdd5DVrf39LTPUV2QACAOAEEAIgTQACAOAEEAIgTQACAuFOtB9CTbSd651zStIaT9klrONWvZupa83y6WK69lt0mo9S+HRAAIE4AAQDiBBAAIE4AAQDiBBAAIG7vLpjWv79+m8RpbyfH6zKfTDVax1nNGq/5fDXv1OEZtdan5t9/NR2yLuyAAABxAggAECeAAABxAggAECeAAABxAggAELd3G+7Ui9pStI4xkpYXVFFfr+22NT9fna3je9tiXHZAAIA4AQQAiBNAAIA4AQQAiBNAAIC4ri6jW9JFSb12DVHX1PWcc6naaLV/CHPeDaN1LvR6GRkZqXrtqWbsgAAAcQIIABAngAAAcQIIABAngAAAcXt3wSQk7lVIdRSMdgKfeWqujXVuzxpwaHP+bpja6TVKR6kdEAAgTgABAOIEEAAgTgABAOIEEAAgTgABAOK6asOt2Z60JK1binuQmAPz3K9RLtdibIlfnzDnM2qOq+YFmidlBwQAiBNAAIA4AQQAiBNAAIA4AQQAiOuqC2aOqZ0LqRPzrT+f7aaugYsF27MGx/OeWY7WnV5zaumk3yU7IABAnAACAMQJIABAnAACAMQJIABA3PBdMNs4Bc42LTsHat53dMjT6Utm3tah178D5tTY1HfAnI6aFvNlBwQAiBNAAIA4AQQAiBNAAIA4AQQAiBNAAIC4xbbhJmjZy0nMda/rOac9bk573lq0noNeL9BkPImW3l1O+p6xAwIAxAkgAECcAAIAxAkgAECcAAIAxO3dBdP6hHbrk+u0teaujtbfvaWpeRldzTVIXVTIdGt4z7RgBwQAiBNAAIA4AQQAiBNAAIA4AQQAiBNAAIC4ri6j0+oE+9v1fdGGOU9i3hKXgc3h/btdzdbtXrVo6bYDAgDECSAAQJwAAgDECSAAQJwAAgDEHbQLZs0XJa3hGWlriTXW+plaXkY3532Z6KhZSpfHPtb0rD2wAwIAxAkgAECcAAIAxAkgAECcAAIAxB20C2bqSew5J7d7PbW85g6gpDXc0UDO1E6QRI2p4928U+toMY92QACAOAEEAIgTQACAOAEEAIgTQACAOAEEAIjbuw030aKj3Yyp1Myy1GxfVxtMtYYL+XpqW7YDAgDECSAAQJwAAgDECSAAQJwAAgDE7d0FM+d0ugvZOI71n2ZN81XzWdfQ0cC6tX43nPS7ZAcEAIgTQACAOAEEAIgTQACAOAEEAIjbuwtmjqmn0Od01LAOva5/61Po2/Q6Xy/EXTBMlaiZxN9Nvb5LDskOCAAQJ4AAAHECCAAQJ4AAAHECCAAQJ4AAAHEnbsOd04K0hkuiarZUjTovibayNbSujbr+1LGGGj+EXd+bbXM6Z66Xvj6HfP/YAQEA4gQQACBOAAEA4gQQACBOAAEA4jZHjtgDAGF2QACAOAEEAIgTQACAOAEEAIgTQACAOAEEAIgTQACAOAEEAIgTQACAuP8Bixb0xVy7bX4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 600x600 with 16 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"DBN\")\n",
        "plot_batched_images(dbn_images, n_layers, save=\"dbn_n_layers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "yDjUX8vT9_hK"
      },
      "source": [
        "# Nombre de caractères"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2Z8PLN89_hK",
        "outputId": "194152ec-3439-447b-fcc8-6901f0699b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Small: ['A', 'B']\n",
            "Medium: ['A', 'B', 'C', 'D']\n",
            "Large: ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n"
          ]
        }
      ],
      "source": [
        "small = ['A', 'B']\n",
        "medium = ['A', 'B', 'C', 'D']\n",
        "large = [chr(i) for i in range(65, 75)]\n",
        "\n",
        "print(\"Small:\", small)\n",
        "print(\"Medium:\", medium)\n",
        "print(\"Large:\", large)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-bA9Y2o9_hK",
        "outputId": "5ada6f12-0ca8-4678-e0f9-1ff1130e961c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Size: small ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 122.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training DBN...\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 122.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:00<00:00, 141.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Size: medium ---\n",
            "Training RBM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 78/100 [00:01<00:00, 54.33it/s]"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "lr = 0.1\n",
        "batch_size = 10\n",
        "q = 200\n",
        "n_layers = [320, 200, 200]\n",
        "\n",
        "rbm_images = []\n",
        "dbn_images = []\n",
        "\n",
        "for size, chars in zip([\"small\", \"medium\", \"large\"], [small, medium, large]):\n",
        "    print(\"--- Size:\", size, \"---\")\n",
        "    X, y = lire_alpha_digit(chars=chars)\n",
        "\n",
        "    X_gen_rbm = RBM_main(\n",
        "        X,\n",
        "        height,\n",
        "        width,\n",
        "        q,\n",
        "        epochs,\n",
        "        lr,\n",
        "        batch_size,\n",
        "    )\n",
        "\n",
        "    idxs = np.random.choice(X_gen_rbm.shape[0], 4, replace=False)\n",
        "    rbm_images.append(X_gen_rbm[idxs])\n",
        "\n",
        "    X_gen_dbn = DBN_main(\n",
        "        X,\n",
        "        n_layers,\n",
        "        height,\n",
        "        width,\n",
        "        epochs,\n",
        "        lr,\n",
        "        batch_size,\n",
        "    )\n",
        "\n",
        "    idxs = np.random.choice(X_gen_dbn.shape[0], 4, replace=False)\n",
        "    dbn_images.append(X_gen_dbn[idxs])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 930
        },
        "id": "FACHV5Og9_hL",
        "outputId": "6dd0e6a8-f05e-4830-a63a-1f61c20c9678"
      },
      "outputs": [],
      "source": [
        "print(\"RBM\")\n",
        "plot_batched_images(rbm_images, [\"small\", \"medium\", \"large\"], save=\"rbm_dataset_size\")\n",
        "\n",
        "print(\"DBN\")\n",
        "plot_batched_images(dbn_images, [\"small\", \"medium\", \"large\"], save=\"dbn_dataset_size\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "QcCYxriu9_hL"
      },
      "source": [
        "# Etude sur MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pd5o6QOm9_hL"
      },
      "outputs": [],
      "source": [
        "if notebook_version=='light':\n",
        "    train_size = 12000 \n",
        "    n_main = 10\n",
        "    n_hidden = 2\n",
        "    pretrain_iter = 10 \n",
        "    train_iter = 20 \n",
        "elif notebook_version=='full':\n",
        "#----------\n",
        "    train_size = 60000 # Default = 30000\n",
        "    n_main = 200 # standard number of neurons per layer (default = 200)\n",
        "    n_hidden = 2 # default = 2\n",
        "    pretrain_iter = 50 # 100 default = 50?\n",
        "    train_iter = 200 # 200 default = 200?\n",
        "\n",
        "# Create directory to save plots\n",
        "savepath = 'save' + '_' + str(train_size) + '_' + str(n_main) + '_' + str(n_hidden) + '_' + str(pretrain_iter) +'_' + str(train_iter) + '/'\n",
        "if not os.path.exists(savepath):\n",
        "    os.makedirs(savepath)\n",
        "    print('Directory \"' + str(savepath) + '\" created')\n",
        "#----------\n",
        "\n",
        "X_train, X_test, y_train, y_test = lire_mnist(subsample_size=train_size) # -1\n",
        "\n",
        "batch_size = 256 # 2000 \n",
        "lr = 0.1\n",
        "\n",
        "n_layers = [784] + [n_main] * n_hidden + [10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "1IpCFaCD9_hL",
        "outputId": "23913fca-be6a-4c17-c27f-fc3f88618c04"
      },
      "outputs": [],
      "source": [
        "plot_data(X_train.reshape(-1, 28, 28), save=\"mnist\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "sQ7RTxqkb9ga",
        "outputId": "5253434e-f74a-4bc0-e28e-282332f83a55"
      },
      "outputs": [],
      "source": [
        "ce, score, train_score, _ = DNN_main(\n",
        "    X_train,\n",
        "    X_test,\n",
        "    y_train,\n",
        "    y_test,\n",
        "    n_layers=[784] + [n_main] * n_hidden + [10], #--------\n",
        "    lr=lr,\n",
        "    batch_size=batch_size,\n",
        "    pretrain_iter=None,\n",
        "    train_iter=train_iter,\n",
        "    show_preds=True,\n",
        "    verbose=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPcp4h5m9_hM",
        "outputId": "fd60b7b6-f659-44f2-f3b6-fd5acd9920d2"
      },
      "source": [
        "#### Fig 1 : 2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de couches (par exemple 2 couches de 200, puis 3 couches de 200, ... puis 5 couches de 200). On utilisera toutes les données d’apprentissage et test;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twOLTI61b9gb",
        "outputId": "0c3d3820-2c85-45df-c19d-f3cfefcb9ee0"
      },
      "outputs": [],
      "source": [
        "acc_1_layer = []\n",
        "acc_2_layer = []\n",
        "acc_1_train_layer = []\n",
        "acc_2_train_layer = []\n",
        "\n",
        "sizes = range(1, 6)\n",
        "\n",
        "for size in sizes:\n",
        "    print(f\"--- {size} hidden layer(s) ---\")\n",
        "    n_layers = [784] + [n_main] * size + [10]  #--------\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_layer.append(score_model1)\n",
        "    acc_1_train_layer.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_layer.append(score_model2)\n",
        "    acc_2_train_layer.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    sizes, \n",
        "    acc_1_layer, \n",
        "    acc_2_layer, \n",
        "    acc_1_train=acc_1_train_layer, \n",
        "    acc_2_train=acc_2_train_layer, \n",
        "    x_label='Number of hidden layers', \n",
        "    top=.2,\n",
        "    save=\"mnist_number_hidden_layers_top\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3msQRPGb9go"
      },
      "source": [
        "#### Fig 2 : 2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de neurones par couches (par exemple 2 couches de 100, puis 2 couches de 300, ...puis 2 couches de 700,...). on utilisera toutes les données d’apprentissage et test;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYt1J04Ab9gp"
      },
      "outputs": [],
      "source": [
        "acc_1_neurons = []\n",
        "acc_2_neurons = []\n",
        "acc_1_train_neurons = []\n",
        "acc_2_train_neurons = []\n",
        "\n",
        "start = 1\n",
        "stop = 6 # reduce 11\n",
        "step = 1 # reduce: 2 ---------------------------------------------------------------\n",
        "n_neurons = range(start * n_main, stop * n_main + 1, step * n_main)\n",
        "\n",
        "for neurons in n_neurons:\n",
        "    print(f\"--- {neurons} neurons by layer, {n_hidden} layers ---\")\n",
        "    n_layers = [784] + [neurons] * n_hidden +  [10]\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_neurons.append(score_model1)\n",
        "    acc_1_train_neurons.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_neurons.append(score_model2)\n",
        "    acc_2_train_neurons.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "len(acc_1_neurons), len(acc_2_neurons), len(acc_1_train_neurons), len(acc_2_train_neurons)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    n_neurons, \n",
        "    acc_1_neurons, \n",
        "    acc_2_neurons, \n",
        "    acc_1_train=acc_1_train_neurons, \n",
        "    acc_2_train=acc_2_train_neurons, \n",
        "    x_label='Number of neurons', \n",
        "    top=.3,\n",
        "    save=\"mnist_number_of_neurons_top\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNF1LvQsb9gp"
      },
      "source": [
        "#### Fig 3 : 2 courbes exprimant le taux d’erreur des 2 réseaux en fonction du nombre de données train (par exemple on fixe 2 couches de 200 puis on utilise 1000 données train, 3000, 7000, 10000, 30000, 60000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVVBa8tkb9gq"
      },
      "outputs": [],
      "source": [
        "acc_1_train_size = []\n",
        "acc_2_train_size = []\n",
        "acc_1_train_size_train = []\n",
        "acc_2_train_size_train = []\n",
        "\n",
        "n_layers = [784] + [2 * n_main] * n_hidden +  [10]\n",
        "\n",
        "samples_sizes = [2000, 6000, 12000, 30000, 60000] # ------------------------------------------\n",
        "# samples_sizes = [1000, 2000, 3000, 60000, 12000, 30000, 60000]\n",
        "\n",
        "for size in samples_sizes:\n",
        "    print(f\"--- {size} samples ---\")\n",
        "    X_train_, X_test_, y_train_, y_test_ = lire_mnist(subsample_size=size)\n",
        "\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train_,\n",
        "        X_test_,\n",
        "        y_train_,\n",
        "        y_test_,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_train_size.append(score_model1)\n",
        "    acc_1_train_size_train.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train_,\n",
        "        X_test_,\n",
        "        y_train_,\n",
        "        y_test_,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_train_size.append(score_model2)\n",
        "    acc_2_train_size_train.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    samples_sizes, \n",
        "    acc_1_train_size, \n",
        "    acc_2_train_size, \n",
        "    acc_1_train=acc_1_train_size_train, \n",
        "    acc_2_train=acc_2_train_size_train, \n",
        "    x_label='Number of training data', \n",
        "    # top=.3,\n",
        "    save=\"mnist_number_of_training_data\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_1_batch_size = []\n",
        "acc_2_batch_size = []\n",
        "acc_1_batch_size_train = []\n",
        "acc_2_batch_size_train = []\n",
        "\n",
        "n_layers = [784] + [2 * n_main] * n_hidden +  [10]\n",
        "\n",
        "\n",
        "batch_sizes = [16, 32, 64, 128, 256, 512] # ------------------------------------------\n",
        "# batch_sizes = [16, 32, 64, 128, 256, 512, 1024, 2048]\n",
        "\n",
        "for batch_size in batch_sizes:\n",
        "    print(f\"--- {batch_size} samples per batch ---\")\n",
        "\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_batch_size.append(score_model1)\n",
        "    acc_1_batch_size_train.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_batch_size.append(score_model2)\n",
        "    acc_2_batch_size_train.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    batch_sizes, \n",
        "    acc_1_batch_size, \n",
        "    acc_2_batch_size, \n",
        "    acc_1_train=acc_1_batch_size_train, \n",
        "    acc_2_train=acc_2_batch_size_train, \n",
        "    x_label='Size of batch', \n",
        "    # top=.2,\n",
        "    save=\"mnist_size_of_batch\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_1_pretrain_iter = []\n",
        "acc_2_pretrain_iter = []\n",
        "acc_1_pretrain_iter_train = []\n",
        "acc_2_pretrain_iter_train = []\n",
        "\n",
        "n_layers = [784] + [2 * n_main] * n_hidden +  [10]\n",
        "\n",
        "\n",
        "pretrain_iter_sizes = [10, 20, 30, 50, 100] # ------------------------------------------\n",
        "# pretrain_iter_sizes = [10, 20, 50, 100, 200, 300]\n",
        "\n",
        "for pretrain_iter_ in pretrain_iter_sizes:\n",
        "    print(f\"--- {pretrain_iter_} iterations per RBM in pretrain ---\")\n",
        "\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter_,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_pretrain_iter.append(score_model1)\n",
        "    acc_1_pretrain_iter_train.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_pretrain_iter.append(score_model2)\n",
        "    acc_2_pretrain_iter_train.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    pretrain_iter_sizes, \n",
        "    acc_1_pretrain_iter, \n",
        "    # acc_2_pretrain_iter, \n",
        "    acc_1_train=acc_1_pretrain_iter_train, \n",
        "    # acc_2_train=acc_2_pretrain_iter_train, \n",
        "    x_label='Number of iterations pretrain', \n",
        "    # top=.1,\n",
        "    save=\"mnist_number_of_pretrain_iter\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_1_train_iter = []\n",
        "acc_2_train_iter = []\n",
        "acc_1_train_iter_train = []\n",
        "acc_2_train_iter_train = []\n",
        "\n",
        "n_layers = [784] + [2 * n_main] * n_hidden +  [10]\n",
        "\n",
        "if notebook_version=='light':\n",
        "    train_iter_sizes = [20, 50, 100, 200]\n",
        "if notebook_version=='full':\n",
        "    train_iter_sizes = [20, 50, 100, 200, 300, 500, 700, 1000] # ------------------------------------------\n",
        "\n",
        "\n",
        "for train_iter_ in train_iter_sizes:\n",
        "    print(f\"--- {train_iter_} iterations in DNN train ---\")\n",
        "\n",
        "    \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "    ce_model1, score_model1, train_score_model1, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=pretrain_iter,\n",
        "        train_iter=train_iter_,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_1_train_iter.append(score_model1)\n",
        "    acc_1_train_iter_train.append(train_score_model1)\n",
        "\n",
        "    \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "    ce_model2, score_model2, train_score_model2, _ = DNN_main(\n",
        "        X_train,\n",
        "        X_test,\n",
        "        y_train,\n",
        "        y_test,\n",
        "        n_layers=n_layers,\n",
        "        lr=lr,\n",
        "        batch_size=batch_size,\n",
        "        pretrain_iter=False,\n",
        "        train_iter=train_iter_,\n",
        "        verbose=1,\n",
        "    )\n",
        "    acc_2_train_iter.append(score_model2)\n",
        "    acc_2_train_iter_train.append(train_score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_DNN(\n",
        "    train_iter_sizes, \n",
        "    acc_1_train_iter, \n",
        "    acc_2_train_iter, \n",
        "    acc_1_train=acc_1_train_iter_train, \n",
        "    acc_2_train=acc_2_train_iter_train, \n",
        "    x_label='Number of iterations DNN', \n",
        "    save=\"mnist_number_of_train_iter\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_main, n_hidden, pretrain_iter, train_iter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WVTYjexb9gq"
      },
      "source": [
        "#### Resultat avec les meilleurs paramètres\n",
        "\n",
        "A CALIBRER APRES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mi58AHXb9gr"
      },
      "outputs": [],
      "source": [
        "# \"\"\"-------------- Model 1 : pre-train + train -------------------\"\"\"\n",
        "\n",
        "# if notebook_version=='full':\n",
        "#     X_train_f, X_test, y_train_f, y_test = lire_mnist(subsample_size=-1)\n",
        "#     n_layers_b = [784, 700, 700, 10]\n",
        "#     pretrain_iter_b = 50\n",
        "#     train_iter_b = 1000\n",
        "#     batch_size_b = 32\n",
        "\n",
        "#     ce_model1, score_model1, train_score_model1, model1 = DNN_main(\n",
        "#         X_train_f,\n",
        "#         X_test,\n",
        "#         y_train_f,\n",
        "#         y_test,\n",
        "#         n_layers=n_layers_b,\n",
        "#         lr=lr,\n",
        "#         batch_size=batch_size_b,\n",
        "#         pretrain_iter=pretrain_iter_b,\n",
        "#         train_iter=train_iter_b,\n",
        "#         verbose=1,\n",
        "#     )\n",
        "\n",
        "#     print(\"----------  Resultat model 1 : pré-entraînément + entraînément  --------- \")\n",
        "#     print(\"            cross_entropie : \", ce_model1)\n",
        "#     print(\"            Accuracy : \", score_model1)\n",
        "#     print(\"       \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# if notebook_version=='full':\n",
        "#   # ----------\n",
        "#   train_size_b = 60000 # Default = 30000\n",
        "#   n_main_b = 700 # standard number of neurons per layer (default = 100)\n",
        "#   n_hidden_b = 2 # default = 2\n",
        "#   pretrain_iter = pretrain_iter_b # 100 default = 50?\n",
        "#   train_iter = train_iter_b # 200 default = 200?\n",
        "\n",
        "#   # Create directory to save plots\n",
        "#   savepath_b = 'save' + '_' + str(train_size_b) + '_' + str(n_main_b) + '_' + str(n_hidden_b) + '_' + str(pretrain_iter_b) +'_' + str(train_iter_b) + '/'\n",
        "#   if not os.path.exists(savepath):\n",
        "#       os.makedirs(savepath)\n",
        "#       print('Directory \"' + str(savepath) + '\" created')\n",
        "#   import pickle\n",
        "#   with open(savepath_b + 'best_model_pretrain.pkl', 'wb') as f:\n",
        "#     pickle.dump(model1, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# with open(savepath_b + 'best_model_pretrain.pkl', 'rb') as f:\n",
        "#   model1_ = pickle.load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# \"\"\"-------------- Model 2 : train only ---------------------------\"\"\"\n",
        "\n",
        "# X_train_f, X_test, y_train_f, y_test = lire_mnist(subsample_size=-1)\n",
        "# n_layers_b2 = [784, 100, 100, 10]\n",
        "# train_iter_b = 100\n",
        "# batch_size_b = 256\n",
        "# pretrain_iter = False\n",
        "# ce_model2, score_model2, train_score_model2, model2 = DNN_main(\n",
        "#     X_train_f,\n",
        "#     X_test,\n",
        "#     y_train_f,\n",
        "#     y_test,\n",
        "#     n_layers=n_layers_b2,\n",
        "#     lr=lr,\n",
        "#     batch_size=batch_size_b,\n",
        "#     pretrain_iter=pretrain_iter,\n",
        "#     train_iter=train_iter_b,\n",
        "#     verbose=1,\n",
        "# )\n",
        "\n",
        "# print(\"----------  Resultat model 2 : uniquement entrainé  --------- \")\n",
        "# print(\"            cross_entropie : \", ce_model2)\n",
        "# print(\"            Accuracy : \", score_model2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_ = X_test\n",
        "y_ = y_test\n",
        "# X_ = X_train_f\n",
        "# y_ = y_train_f\n",
        "y_pred, _ = test_DNN(model1_, X_, y_)\n",
        "test_score = accuracy_score(np.argmax(y_, axis=1), np.argmax(y_pred, axis=1))\n",
        "print(test_score)\n",
        "plot_preds(X_, y_, y_pred, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "iM3lLZdv9_iP"
      },
      "source": [
        "# VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVzze_gV9_iQ"
      },
      "outputs": [],
      "source": [
        "# X_train, X_test, y_train, y_test = lire_mnist(subsample_size=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKURK3DJ9_iQ"
      },
      "outputs": [],
      "source": [
        "# class VAE(nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, latent_dim):\n",
        "#         super(VAE, self).__init__()\n",
        "\n",
        "#         self.input_dim = input_dim\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.latent_dim = latent_dim\n",
        "\n",
        "#         self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "#         self.fc21 = nn.Linear(hidden_dim, latent_dim)\n",
        "#         self.fc22 = nn.Linear(hidden_dim, latent_dim)\n",
        "#         self.fc3 = nn.Linear(latent_dim, hidden_dim)\n",
        "#         self.fc4 = nn.Linear(hidden_dim, input_dim)\n",
        "\n",
        "#     def encode(self, x):\n",
        "#         h1 = F.relu(self.fc1(x))\n",
        "#         return self.fc21(h1), self.fc22(h1)\n",
        "\n",
        "#     def reparameterize(self, mu, logvar):\n",
        "#         std = torch.exp(0.5 * logvar)\n",
        "#         eps = torch.randn_like(std)\n",
        "#         return eps.mul(std).add_(mu)\n",
        "\n",
        "#     def decode(self, z):\n",
        "#         h3 = F.relu(self.fc3(z))\n",
        "#         return torch.sigmoid(self.fc4(h3))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         mu, logvar = self.encode(x.view(-1, self.input_dim))\n",
        "#         z = self.reparameterize(mu, logvar)\n",
        "#         return self.decode(z), mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMNzZfgu9_iR"
      },
      "outputs": [],
      "source": [
        "# def loss_function(recon_x, x, mu, logvar):\n",
        "#     BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction=\"sum\")\n",
        "\n",
        "#     KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "#     return BCE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aePGRcBN9_iR"
      },
      "outputs": [],
      "source": [
        "# def train(model, device, train_loader, optimizer, epoch, verbose=1):\n",
        "#     model.train()\n",
        "#     train_loss = 0\n",
        "#     for batch_idx, (data, _) in enumerate(train_loader):\n",
        "#         data = data.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         recon_batch, mu, logvar = model(data)\n",
        "#         loss = loss_function(recon_batch, data, mu, logvar)\n",
        "#         loss.backward()\n",
        "#         train_loss += loss.item()\n",
        "#         optimizer.step()\n",
        "#         if batch_idx % 100 == 0 and verbose >= 2:\n",
        "#             print(\n",
        "#                 \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "#                     epoch,\n",
        "#                     batch_idx * len(data),\n",
        "#                     len(train_loader.dataset),\n",
        "#                     100.0 * batch_idx / len(train_loader),\n",
        "#                     loss.item() / len(data),\n",
        "#                 )\n",
        "#             )\n",
        "#     if verbose >= 1:\n",
        "#         print(\n",
        "#             \"====> Epoch: {} Average loss: {:.4f}\".format(\n",
        "#                 epoch, train_loss / len(train_loader.dataset)\n",
        "#             )\n",
        "#         )\n",
        "\n",
        "\n",
        "# def test(model, device, test_loader, verbose=1):\n",
        "#     model.eval()\n",
        "#     test_loss = 0\n",
        "#     with torch.no_grad():\n",
        "#         for i, (data, _) in enumerate(test_loader):\n",
        "#             data = data.to(device)\n",
        "#             recon_batch, mu, logvar = model(data)\n",
        "#             test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "#             if i == 0:\n",
        "#                 n = min(data.size(0), 8)\n",
        "#                 comparison = torch.cat(\n",
        "#                     [data[:n], recon_batch.view(batch_size, 1, 28, 28)[:n]]\n",
        "#                 )\n",
        "#                 save_image(\n",
        "#                     comparison.cpu(),\n",
        "#                     \"reconstruction.png\",\n",
        "#                     nrow=n,\n",
        "#                 )\n",
        "\n",
        "#     test_loss /= len(test_loader.dataset)\n",
        "#     if verbose >= 1:\n",
        "#         print(\"====> Test set loss: {:.4f}\".format(test_loss))\n",
        "\n",
        "\n",
        "# def VAE_main(train_loader, test_loader, batch_size, epochs, lr):\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     kwargs = {\"num_workers\": 1, \"pin_memory\": True} if torch.cuda.is_available() else {}\n",
        "\n",
        "#     model = VAE(784, 400, 20).to(device)\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "#     for epoch in tqdm(range(1, epochs + 1)):\n",
        "#         train(model, device, train_loader, optimizer, epoch)\n",
        "#         test(model, device, test_loader)\n",
        "#         with torch.no_grad():\n",
        "#             sample = torch.randn(25, 20).to(device)\n",
        "#             sample = model.decode(sample).cpu()\n",
        "#             save_image(\n",
        "#                 sample.view(25, 1, 28, 28),\n",
        "#                 \"sample_\" + str(epoch) + \".png\",\n",
        "#             )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fSxo3hsDZq2"
      },
      "outputs": [],
      "source": [
        "# mnist_train = datasets.MNIST(\n",
        "#     \"data/\", train=True, download=True, transform=transforms.ToTensor()\n",
        "# )\n",
        "# mnist_test = datasets.MNIST(\n",
        "#     \"data/\", train=False, download=True, transform=transforms.ToTensor()\n",
        "# )\n",
        "# train_loader = torch.utils.data.DataLoader(\n",
        "#     mnist_train,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True,\n",
        "# )\n",
        "# test_loader = torch.utils.data.DataLoader(\n",
        "#     mnist_test,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True,\n",
        "# )\n",
        "\n",
        "# VAE_main(train_loader, test_loader, batch_size=128, epochs=10, lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ7N-AauHtV5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "fdc5d049eec9eaeec1c766624dece239ed4d1607671b6d493e340e6a292a2d6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
